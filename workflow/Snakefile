from pathlib import Path
from lib.query_filter import validate_column, query_dicoms, post_filter


configfile: "config/config.yml"


# Load stages dict
stages = config.get("stages", {})

stage_query = Path(stages.get("query", "results/0_query"))
stage_filter = Path(stages.get("filter", "results/1_filter"))
stage_download = Path(stages.get("download", "results/2_download"))
stage_convert = Path(stages.get("convert", "results/3_convert"))
stage_fix = Path(stages.get("fix", "results/4_fix"))

final_bids_dir = Path(config.get("final_bids_dir", "bids"))

# ----------
# 0 - query
# ----------
df = query_dicoms(
    config["search_specs"],
    credentials_file=config["credentials_file"],
    **config.get("query_kwargs", dict()),
)

# save unfiltered list
stage_query.mkdir(parents=True, exist_ok=True)
df.to_csv(stage_query / "studies.tsv", sep="\t", index=False)

# Validate both columns
valid_subject = validate_column(df, "subject")
valid_session = validate_column(df, "session")

# ----------
# 1 - filter
# ----------
df = post_filter(df, config["study_filter_specs"]).sort_values(
    by=["subject", "session"]
)


# use --config subject= to process particular subject
if config.get("subject", False):
    df = df[df.subject == config["subject"]]

# use --config subject= to process particular subject
if config.get("session", False):
    df = df[df.session == config["session"]]

# use --config head=1  to only process first subject
head_n = config.get("head", 0)
if head_n > 0:
    df = df.head(head_n)


# write this out so user can inspect
stage_filter.mkdir(parents=True, exist_ok=True)
df.to_csv(stage_filter / "studies_filtered.tsv", sep="\t", index=False)

# lists for target rules (duplicates dropped)
subjects = df[["subject", "session"]].drop_duplicates().subject
sessions = df[["subject", "session"]].drop_duplicates().session


rule all:
    input:
        final_bids_dir,
        stage_convert / "qc/aggregate_report.html",
        stage_fix / "qc/aggregate_report.html",


rule download:
    input:
        expand(
            stage_download / "dicoms/sub-{subject}/ses-{session}",
            zip,
            subject=subjects,
            session=sessions,
        ),


rule convert:
    input:
        validator_json=stage_convert / "qc/bids_validator.json",


rule fix:
    input:
        validator_json=stage_fix / "qc/bids_validator.json",
        aggregate_report=stage_fix / "qc/aggregate_report.html",


rule qc:
    input:
        expand(
            stage_convert
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.svg",
            zip,
            subject=subjects,
            session=sessions,
        ),


def get_uid_from_wildcards(wildcards):
    matches = lookup(
        query=f"subject == '{wildcards.subject}' and session == '{wildcards.session}'",
        within=df,
    )
    if isinstance(matches, tuple):
        return f"-u {matches.StudyInstanceUID}"
    else:
        ids = [m.StudyInstanceUID for m in matches]
        if config["merge_duplicate_studies"]:
            print(f"WARNING: Merging multiple matches for {wildcards}")
            return " ".join([f"-u {i}" for i in ids])
        else:
            bad_ids = "\n".join(ids)
            raise LookupError(
                f'Multiple studies matched for {wildcards}: \n{bad_ids}\nThese can be merged instead by using setting in the config "merge_duplicate_studies: true" '
            )


rule cfmm2tar:
    params:
        uid=get_uid_from_wildcards,
        cfmm2tar_download_options=config["cfmm2tar_download_options"],
        creds_file=config["credentials_file"],
    output:
        dicoms_dir=directory(stage_download / "dicoms/sub-{subject}/ses-{session}"),
    log:
        "logs/download/sub-{subject}_ses-{session}.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=15,
    shell:
        "cfmm2tar -c {params.creds_file} {params.cfmm2tar_download_options} {params.uid} {output.dicoms_dir} &> {log}"


rule heudiconv:
    """
    Run heudiconv to convert DICOMs to BIDS format.
    
    Automatically handles both single-study and multi-study cases:
    - Single study: runs heudiconv directly on the DICOM directory
    - Multiple studies (when merge_duplicate_studies=true): processes each tar file
      separately with heudiconv, then merges outputs with series ID offsetting
    """
    input:
        dicoms_dir=rules.cfmm2tar.output.dicoms_dir,
        heuristic=config["heuristic"],
        base_heuristics=config.get("base_heuristics", []),
        dcmconfig_json=config["dcmconfig_json"],
    params:
        heudiconv_options=config["heudiconv_options"],
        out_info_dir=lambda wildcards: stage_convert
        / "qc/sub-{subject}/ses-{session}".format(**wildcards),
    output:
        bids_subj_dir=directory(
            stage_convert / "bids-staging/sub-{subject}/ses-{session}"
        ),
        auto_txt=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_auto.txt",
        dicominfo_tsv=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_dicominfo.tsv",
    log:
        "logs/heudiconv/sub-{subject}_ses-{session}.log",
    shadow:
        "minimal"
    threads: 16
    resources:
        mem_mb=16000,
        runtime=30,
    group:
        "convert"
    script:
        "scripts/run_heudiconv.py"


rule bidsignore_convert_staging:
    params:
        lines=config.get("bidsignore", []),
    output:
        ignore=stage_convert / "bids-staging/.bidsignore",
    resources:
        mem_mb=4000,
        runtime=10,
    run:
        with open(output.ignore, "w") as file:
            for item in params.lines:
                file.write(item + "\n")


rule dataset_description_for_convert_staging:
    input:
        "resources/dataset_description.json",
    output:
        stage_convert / "bids-staging/dataset_description.json",
    log:
        "logs/dataset_description/init_dataset_description_staging.log",
    resources:
        mem_mb=4000,
        runtime=1,
    shell:
        "cp {input} {output} &> {log}"


rule generate_convert_qc_figs:
    input:
        auto_txt=rules.heudiconv.output.auto_txt,
        dicominfo_tsv=rules.heudiconv.output.dicominfo_tsv,
    output:
        series_list=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.svg",
        series_tsv=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.tsv",
        unmapped=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_unmapped.svg",
    log:
        "logs/generate_convert_qc_figs/sub-{subject}_ses-{session}.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    group:
        "convert"
    script:
        "scripts/generate_convert_qc_figs.py"


def get_cmd_cp_subj_dirs(wildcards, input, output):

    cmds = []
    for subj_dir in input.bids_subj_dirs:
        # get path to create at output
        subj_path = Path(subj_dir)
        rel_path = subj_path.relative_to(subj_path.parents[1])
        out_path = Path(output.bids_dir) / rel_path
        cmds.append("mkdir -p {out_path}")
        cmds.append("cp -R {subj_path}/* {out_path}/")

    return " ".join(cmds)


rule assemble_convert_bids:
    """
    Assemble the converted BIDS dataset from staging directory.
    This ensures the BIDS directory is always clean and matches the requested subjects.
    """
    input:
        dataset_description=stage_convert / "bids-staging/dataset_description.json",
        ignore=stage_convert / "bids-staging/.bidsignore",
        bids_subj_dirs=expand(
            stage_convert / "bids-staging/sub-{subject}/ses-{session}",
            zip,
            subject=subjects,
            session=sessions,
        ),
        convert_qc=expand(
            stage_convert
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.svg",
            zip,
            subject=subjects,
            session=sessions,
        ),
    params:
        cmd_cp_subj_dirs=get_cmd_cp_subj_dirs,
    output:
        bids_dir=directory(stage_convert / "bids"),
    threads: 1
    resources:
        mem_mb=4000,
        runtime=60,
    log:
        "logs/assemble_convert_bids/assemble.log",
    shell:
        "mkdir -p {output.bids_dir} && "
        "cp {input.dataset_description} {output.bids_dir} && "
        "{params.cmd_cp_subj_dirs}"


rule validate_converted:
    input:
        bids_dir=stage_convert / "bids",
    output:
        validator_json=stage_convert / "qc/bids_validator.json",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    log:
        "logs/convert/validator.txt",
    shell:
        # || true prevents workflow failure when validation errors are found
        # validate twice, to write to log, to txt, and to json
        "bids-validator-deno {input.bids_dir} --format text &> {log} || true && "
        "bids-validator-deno {input.bids_dir} --format json_pp -o {output.validator_json} || true"


rule post_convert_fix:
    """
    Copy BIDS dataset and apply post-conversion fixes
    (defined in config['post_bids_fixes']) sequentially.
    """
    input:
        bids_subj_dir=stage_convert / "bids-staging/sub-{subject}/ses-{session}",
        validator_json=stage_convert / "qc/bids_validator.json",
    output:
        bids_subj_dir=directory(stage_fix / "bids-staging/sub-{subject}/ses-{session}"),
        prov_json=stage_fix
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_provenance.json",
    params:
        fixes=config["post_convert_fixes"],
    log:
        "logs/post_convert_fix/sub-{subject}_ses-{session}.log",
    threads: 1
    resources:
        mem_mb=8000,
        runtime=60,
    script:
        "scripts/post_convert_fix.py"


rule bidsignore_post_convert_fix_staging:
    params:
        lines=config.get("bidsignore", []),
    output:
        ignore=stage_fix / "bids-staging/.bidsignore",
    resources:
        mem_mb=4000,
        runtime=10,
    run:
        with open(output.ignore, "w") as file:
            for item in params.lines:
                file.write(item + "\n")


rule dataset_description_fixed_staging:
    input:
        dd_json=stage_convert / "bids-staging/dataset_description.json",
    output:
        dd_json=stage_fix / "bids-staging/dataset_description.json",
    log:
        "logs/fix/dataset_description_staging.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    shell:
        "cp {input.dd_json} {output.dd_json} &> {log}"


rule assemble_fix_bids:
    """
    Assemble the fixed BIDS dataset from staging directory.
    This ensures the BIDS directory is always clean and matches the requested subjects.
    """
    input:
        dataset_description=stage_fix / "bids-staging/dataset_description.json",
        ignore=stage_fix / "bids-staging/.bidsignore",
        bids_subj_dirs=expand(
            stage_fix / "bids-staging/sub-{subject}/ses-{session}",
            zip,
            subject=subjects,
            session=sessions,
        ),
        prov_jsons=expand(
            stage_fix
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_provenance.json",
            zip,
            subject=subjects,
            session=sessions,
        ),
    params:
        cmd_cp_subj_dirs=get_cmd_cp_subj_dirs,
    output:
        bids_dir=directory(stage_fix / "bids"),
    threads: 1
    resources:
        mem_mb=4000,
        runtime=60,
    log:
        "logs/assemble_fix_bids/assemble.log",
    shell:
        "mkdir -p {output.bids_dir} && "
        "cp {input.dataset_description} {output.bids_dir} && "
        "cp {input.ignore} {output.bids_dir} && "
        "{params.cmd_cp_subj_dirs}"


rule validate_fixed:
    input:
        bids_dir=stage_fix / "bids",
    output:
        validator_json=stage_fix / "qc/bids_validator.json",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    log:
        "logs/fix/validator.txt",
    shell:
        # || true prevents workflow failure when validation errors are found
        # validate twice, to write to log, to txt, and to json
        "bids-validator-deno {input.bids_dir} --format text &> {log} || true && "
        "bids-validator-deno {input.bids_dir} --format json_pp -o {output.validator_json} || true"


rule subject_report:
    """Generate individual QC report for a single subject/session"""
    input:
        series_tsv=stage_convert
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.tsv",
        provenance_json=stage_fix
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_provenance.json",
    output:
        html_report=stage_fix
        / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_report.html",
    log:
        "logs/subject_report/sub-{subject}_ses-{session}.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    script:
        "scripts/generate_subject_report.py"


rule aggregate_report_convert:
    """Generate aggregate report with TOC linking to all subject reports"""
    input:
        series_tsv=expand(
            stage_convert
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.tsv",
            zip,
            subject=subjects,
            session=sessions,
        ),
        subject_reports=expand(
            stage_fix
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_report.html",
            zip,
            subject=subjects,
            session=sessions,
        ),
        convert_validator_json=stage_convert / "qc/bids_validator.json",
    output:
        html_report=stage_convert / "qc/aggregate_report.html",
    log:
        "logs/aggregate_report/aggregate_report_convert.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    script:
        "scripts/generate_aggregate_all_report.py"


rule aggregate_report_fix:
    """Generate aggregate report with TOC linking to all subject reports"""
    input:
        series_tsv=expand(
            stage_convert
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_series.tsv",
            zip,
            subject=subjects,
            session=sessions,
        ),
        subject_reports=expand(
            stage_fix
            / "qc/sub-{subject}/ses-{session}/sub-{subject}_ses-{session}_report.html",
            zip,
            subject=subjects,
            session=sessions,
        ),
        fix_validator_json=stage_fix / "qc/bids_validator.json",
    output:
        html_report=stage_fix / "qc/aggregate_report.html",
    log:
        "logs/aggregate_report/aggregate_report_fix.log",
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    script:
        "scripts/generate_aggregate_all_report.py"


rule validate_final:
    """final validation before copying, fails if invalid"""
    input:
        validator_json=stage_fix / "qc/bids_validator.json",
        bids_dir=stage_fix / "bids",
    output:
        validator_txt=stage_fix / "qc/final_bids_validator.txt",  # require validated
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    shell:
        "bids-validator-deno {input.bids_dir} --format text &> {output.validator_txt}"


rule assemble_final_bids:
    """
    Assemble the final BIDS dataset by copying from the fixed BIDS directory.
    This ensures the final BIDS directory is always clean and matches the requested subjects.
    """
    input:
        validator_txt=stage_fix / "qc/final_bids_validator.txt",  # require validated
        bids_dir=stage_fix / "bids",
    output:
        bids_dir=directory(final_bids_dir),
    threads: 1
    resources:
        mem_mb=4000,
        runtime=10,
    log:
        "logs/assemble_final_bids/assemble.log",
    shell:
        "cp -R {input.bids_dir} {output.bids_dir} &> {log}"
